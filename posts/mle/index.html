<!DOCTYPE html>
<html lang="en">

<head>
  <title>
  Maximum Likelihood Estimation · home
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="description" content="Maximum Likelihood Estimation

  Likelihood
  
    
    Link to heading
  

Likelihood describes the joint probability of observed data, y, as a function of the parameters, $\theta$, of a statistical model. The likelihood is NOT a probability density function of the parameters themselves. Instead, it is a function that measures how likely the observed data is for different parameter values. An intuitive way to think about likelihood is that it represents how &ldquo;believable&rdquo; the data is under different model parameters. For example, in a normal distribution, the likelihood tells you how probable it is to observe a specific dataset given the mean and variance values of the distribution.">
<meta name="keywords" content="">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Maximum Likelihood Estimation">
  <meta name="twitter:description" content="Maximum Likelihood Estimation Likelihood Link to heading Likelihood describes the joint probability of observed data, y, as a function of the parameters, $\theta$, of a statistical model. The likelihood is NOT a probability density function of the parameters themselves. Instead, it is a function that measures how likely the observed data is for different parameter values. An intuitive way to think about likelihood is that it represents how “believable” the data is under different model parameters. For example, in a normal distribution, the likelihood tells you how probable it is to observe a specific dataset given the mean and variance values of the distribution.">

<meta property="og:url" content="https://dwil2444.github.io/posts/mle/">
  <meta property="og:site_name" content="home">
  <meta property="og:title" content="Maximum Likelihood Estimation">
  <meta property="og:description" content="Maximum Likelihood Estimation Likelihood Link to heading Likelihood describes the joint probability of observed data, y, as a function of the parameters, $\theta$, of a statistical model. The likelihood is NOT a probability density function of the parameters themselves. Instead, it is a function that measures how likely the observed data is for different parameter values. An intuitive way to think about likelihood is that it represents how “believable” the data is under different model parameters. For example, in a normal distribution, the likelihood tells you how probable it is to observe a specific dataset given the mean and variance values of the distribution.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2022-05-08T00:00:00+00:00">
    <meta property="article:modified_time" content="2022-05-08T00:00:00+00:00">




<link rel="canonical" href="https://dwil2444.github.io/posts/mle/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.ed30115a76cdaa62f2229e973d5b1c89b2d3dd4b1d9c07a729baad06aa3b0cbe.css" integrity="sha256-7TARWnbNqmLyIp6XPVscibLT3UsdnAenKbqtBqo7DL4=" crossorigin="anonymous" media="screen" />








 




<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>




<body class="preload-transitions colorscheme-light">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="https://dwil2444.github.io/">
      home
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/about">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/publications">Publications</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/teaching">Teaching</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/cv.pdf">CV</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="https://dwil2444.github.io/posts/mle/">
              Maximum Likelihood Estimation
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2022-05-08T00:00:00Z">
                May 8, 2022
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              6-minute read
            </span>
          </div>
          
          
          
        </div>
      </header>

      <div class="post-content">
        
        <h1 align="center">Maximum Likelihood Estimation</h1>
<h5 id="likelihood">
  Likelihood
  <a class="heading-link" href="#likelihood">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h5>
<p><strong>Likelihood</strong> describes the joint probability of observed data, <strong>y</strong>, as a function of the parameters, <strong>$\theta$</strong>, of a statistical model. The likelihood is <strong>NOT</strong> a <a href="https://en.wikipedia.org/wiki/Probability_density_function"  class="external-link" target="_blank" rel="noopener">probability density function</a> of the parameters themselves. Instead, it is a function that measures how likely the observed data is for different parameter values. An intuitive way to think about likelihood is that it represents how &ldquo;believable&rdquo; the data is under different model parameters. For example, in a normal distribution, the likelihood tells you how probable it is to observe a specific dataset given the mean and variance values of the distribution.</p>
<p><strong>$\mathcal{L}$ is a probability density over $ y$, not $ \theta $.</strong></p>
<h5 id="likelihood-vs-density-functionposterior-probability">
  Likelihood vs Density Function/Posterior Probability:
  <a class="heading-link" href="#likelihood-vs-density-functionposterior-probability">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h5>
<p>This terminology can be confusing due to similarities with other concepts:</p>
<ul>
<li>
<p>$\mathcal{L} (\theta \mid y) $: Computes a density over $ y $ when parameters $ \theta $ are fixed, with the goal of maximizing the probability of observing $ y $. It represents the <strong>likelihood</strong> of the observed data given the parameters.</p>
</li>
<li>
<p>$ p (\theta \mid y) $: Computes the <strong>posterior probability</strong> in Bayesian inference, giving the likelihood of the parameters $ \theta $ given the observed outcome $ y $. This is found using <strong>Bayes&rsquo; Theorem</strong>, which incorporates prior knowledge of the parameters along with the likelihood of the data.</p>
</li>
</ul>
<h5 id="maximum-likelihood-estimation">
  Maximum Likelihood Estimation:
  <a class="heading-link" href="#maximum-likelihood-estimation">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h5>
<p>Maximum Likelihood Estimation (MLE) aims to find the <strong>values of the parameters</strong> of the statistical model under which the observed data is most probable. In other words, MLE adjusts the model parameters until the model most &ldquo;believes&rdquo; the data could have occurred. We do this by maximizing the <strong>likelihood function</strong>. The resulting parameters are the ones that maximize the probability of observing the given data:
$$ \hat{\theta} = \arg \max_{\theta \in \Theta} \mathcal{L} (\theta \mid y) $$</p>
<h5 id="intuition-behind-mle">
  Intuition behind MLE:
  <a class="heading-link" href="#intuition-behind-mle">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h5>
<p>Consider a simple example where you are trying to fit a normal distribution to a dataset. The parameters of the normal distribution are the mean, $ \mu $, and the variance, $ \sigma^2 $. MLE helps find the values of $ \mu $ and $ \sigma^2 $ that maximize the probability of the observed data given the model. For instance, if you have a dataset of test scores, MLE will estimate the mean and variance that make those scores most likely, according to the normal distribution.</p>
<h5 id="likelihood-function">
  Likelihood Function:
  <a class="heading-link" href="#likelihood-function">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h5>
<p>The likelihood function is the joint probability density of the outcomes $ Y_1, Y_2, \dots, Y_n $, observed under the assumption that these outcomes are independent and parameterized by the model parameters $ \theta_1, \theta_2, \dots, \theta_m $. It is defined as:
$$ L(\theta_1, \dots, \theta_m)  = \prod_{i=1}^{n} f(y_i; \theta_1, \dots, \theta_m) $$</p>
<p>This is a product of the probabilities of each observation, under the assumption that the observations are independent. Each observation is treated as being parameterized by different model parameters. If we assume that each observation follows the same distribution, we multiply the individual likelihoods to get the overall likelihood.</p>
<h5 id="a-quick-note-on-log-likelihood">
  A Quick Note on Log-Likelihood:
  <a class="heading-link" href="#a-quick-note-on-log-likelihood">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h5>
<p>The <strong>log-likelihood</strong> is the natural logarithm of the likelihood function. Since the logarithm is a monotonically increasing function, maximizing the log-likelihood is equivalent to maximizing the likelihood. Taking the logarithm has two key advantages:</p>
<ol>
<li>
<p>It converts the product of probabilities into a sum, making the math easier to handle:
$$ \log \left( \prod_{i=1}^{n} f(y_i; \theta_1, \dots, \theta_m) \right) = \sum_{i=1}^{n} \log f(y_i; \theta_1, \dots, \theta_m) $$</p>
</li>
<li>
<p>The natural logarithm ($\ln$) is easier to differentiate, making it more convenient for optimization.</p>
</li>
</ol>
<p>Thus, the <strong>log-likelihood</strong> is commonly used in practice to simplify the calculation and make optimization more efficient.</p>
<h3 id="intuitive-example-estimating-the-mean-of-a-normal-distribution">
  Intuitive Example: Estimating the Mean of a Normal Distribution
  <a class="heading-link" href="#intuitive-example-estimating-the-mean-of-a-normal-distribution">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Let’s consider a simple example: suppose you have a dataset of test scores $ y_1, y_2, \dots, y_n $, and you want to estimate the mean $ \mu $ of the underlying normal distribution. The likelihood function for the mean, assuming a normal distribution, is given by:
$$ \mathcal{L} (\mu \mid y) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left( -\frac{(y_i - \mu)^2}{2\sigma^2} \right) $$</p>
<p>To make this easier to work with, we take the logarithm:
$$ \log \mathcal{L} (\mu \mid y) = \sum_{i=1}^{n} \left( -\frac{(y_i - \mu)^2}{2\sigma^2} - \frac{1}{2} \log(2\pi \sigma^2) \right) $$</p>
<h5 id="maximizing-the-log-likelihood">
  Maximizing the Log-Likelihood
  <a class="heading-link" href="#maximizing-the-log-likelihood">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h5>
<p>After we&rsquo;ve taken the logarithm of the likelihood function, we have a much easier expression to work with:</p>
<p>$$
\log \mathcal{L} (\mu \mid y) = \sum_{i=1}^{n} \left( -\frac{(y_i - \mu)^2}{2\sigma^2} - \frac{1}{2} \log(2\pi \sigma^2) \right)
$$</p>
<p>Here, the first term represents the likelihood of observing the data, given the mean $ \mu $, and the second term is constant with respect to $ \mu $, as it only involves $ \sigma^2 $, the variance.</p>
<p>Now, let&rsquo;s break down the steps to find the <strong>Maximum Likelihood Estimator (MLE)</strong> for the mean $ \mu $.</p>
<ol>
<li>
<p><strong>Differentiate the Log-Likelihood:</strong>
To find the maximum of the log-likelihood, we take the derivative of $ \log \mathcal{L} (\mu \mid y) $ with respect to $ \mu $. The derivative tells us how the log-likelihood changes as $ \mu $ changes, and we want to find the value of $ \mu $ that maximizes this function.</p>
<p>$$
\frac{\partial}{\partial\mu} \left[ \sum_{i=1}^{n} \left( -\frac{(y_i - \mu)^2}{2\sigma^2} - \frac{1}{2} \log(2\pi \sigma^2) \right) \right]
$$</p>
</li>
<li>
<p><strong>Simplify the Expression:</strong>
The second term involving the logarithm $ \log(2\pi \sigma^2) $ doesn&rsquo;t depend on $ \mu $, so its derivative will be zero. The only part we need to focus on is:</p>
<p>$$
\frac{\partial}{\partial \mu} \left( -\frac{(y_i - \mu)^2}{2\sigma^2} \right)
$$</p>
<p>The derivative of $ (y_i - \mu)^2 $ with respect to $ \mu $ is $ -2(y_i - \mu) $, so we get:</p>
<p>$$
\frac{\partial}{\partial \mu} \log \mathcal{L} (\mu \mid y) = \sum_{i=1}^{n} \frac{(y_i - \mu)}{\sigma^2}
$$</p>
</li>
<li>
<p><strong>Set the Derivative Equal to Zero:</strong>
To find the maximum of the log-likelihood, we set the derivative equal to zero:</p>
<p>$$
\sum_{i=1}^{n} \frac{(y_i - \mu)}{\sigma^2} = 0
$$</p>
</li>
<li>
<p><strong>Solve for $ \mu $:</strong>
Now, we solve for $ \mu $. We can factor out $ \frac{1}{\sigma^2} $ since it&rsquo;s a constant, and then simplify:</p>
<p>$$
\sum_{i=1}^{n} (y_i - \mu) = 0
$$</p>
<p>we can distribute the sum, since $\mu$ is not dependent on the index, $i$, the second sum simplifies as follows: $\sum\limits_{i=1}^{n} \mu = n \mu $</p>
<p>$$
\sum_{i=1}^{n} y_i - n\mu = 0
$$</p>
<p>$$
n\mu = \sum_{i=1}^{n} y_i
$$</p>
<p>$$
\hat{\mu} = \frac{1}{n} \sum_{i=1}^{n} y_i
$$</p>
</li>
</ol>
<h3 id="interpretation-of-the-mle-for-the-mean">
  Interpretation of the MLE for the Mean
  <a class="heading-link" href="#interpretation-of-the-mle-for-the-mean">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<ul>
<li>The <strong>MLE for the mean</strong> $ \hat{\mu} $ is simply the <strong>sample average</strong> of the observed data. This makes intuitive sense because the value of $ \mu $ that maximizes the likelihood is the one that best represents the &ldquo;center&rdquo; of the data, which is the sample mean.</li>
<li>In other words, the best estimate for the true mean, $ \mu $, of the population is the average of the observed data points.</li>
</ul>
<p>Thus, the Maximum Likelihood Estimator (MLE) for the mean in this case is the sample mean, which is a well-known result in statistics.</p>
<h5 id="why-use-mle">
  Why Use MLE?
  <a class="heading-link" href="#why-use-mle">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h5>
<ul>
<li><strong>Consistency</strong>: MLE provides estimates that are consistent, meaning that as the sample size grows, the estimated parameters converge to the true values.</li>
<li><strong>Efficiency</strong>: Under certain regularity conditions, MLE estimators achieve the lowest possible variance, making them efficient.</li>
<li><strong>Flexibility</strong>: MLE can be used with a wide variety of statistical models, from simple distributions to complex models with many parameters.</li>
</ul>
<hr />

      </div>


      <footer>
        


        <div id="disqus_thread"></div>
<script>
  window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "mlbytes" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
    
    document.addEventListener('themeChanged', function (e) { 
        if (document.readyState == 'complete') {
          DISQUS.reset({ reload: true, config: disqus_config });
        }
    });
</script>
        
        
        
        
        
        
      </footer>
    </article>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css"
    integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
  
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"
    integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body,
      {
        delimiters: [
          {left: '$$', right: '$$', display:true},
          {left: '$', right: '$', display:false},
          {left: '\\(', right: '\\)', display: false},
          {left: '\\[', right: '\\]', display: true}
        ]
      }
    );"></script>
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
    2025
    
    ·
    
    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js" integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>

</html>
