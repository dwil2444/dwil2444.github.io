<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>home</title>
    <link>https://dwil2444.github.io/posts/vae/</link>
    <description>Recent content on home</description>
    <generator>Hugo</generator>
    <language>en</language>
    <copyright>Â© 2025 by Dane Williamson. &lt;a href=&#39;https://dwil2444.github.io/privacy&#39;&gt;Privacy policy&lt;/a&gt;.</copyright>
    <lastBuildDate>Tue, 10 May 2022 15:10:15 -0400</lastBuildDate>
    <atom:link href="https://dwil2444.github.io/posts/vae/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Variational Autoencoders</title>
      <link>https://dwil2444.github.io/posts/vae/vae/</link>
      <pubDate>Tue, 10 May 2022 15:10:15 -0400</pubDate>
      <guid>https://dwil2444.github.io/posts/vae/vae/</guid>
      <description>&lt;!-- &#xA;#### Inference&#xA;&#xA;#### Generation&#xA;&#xA;#### ELBO&#xA;&#xA;#### Reparameterization Trick --&gt;</description>
    </item>
    <item>
      <title>Variational Inference</title>
      <link>https://dwil2444.github.io/posts/vae/vi/</link>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      <guid>https://dwil2444.github.io/posts/vae/vi/</guid>
      <description>&lt;h1 align=&#34;center&#34;&gt;What is Variational Inference? &lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;Variational Inference&lt;/strong&gt; is a technique used in Bayesian Statistics to approximate &lt;strong&gt;$p ( z \mid x)$&lt;/strong&gt; the conditional density of an unknown variable, &lt;strong&gt;z&lt;/strong&gt; given an observed variable, &lt;strong&gt;x&lt;/strong&gt; through optimization. To find this approximate density:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;select a family of densities, $\mathscr{D}$ over the latent variables. Each member of the family q(z) $\in \mathscr{D}$ is a candidate approximation to the true density.&lt;/li&gt;&#xA;&lt;li&gt;The optimization problem is then to find the member of this family which is closest in &lt;strong&gt;Kullback-Leibler (KL) divergence&lt;/strong&gt; to the conditional density of interest:&#xA;$$&#xA;q^{*} (z) = \underset{q(z) \in \mathscr{D}}{\text{arg min}} \quad \text{KL} (q(z) \mid \mid p(z \mid x))&#xA;$$&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;kullback-leibler-divergence&#34;&gt;&#xA;  Kullback-Leibler Divergence&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#kullback-leibler-divergence&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;p&gt;The &lt;strong&gt;divergence&lt;/strong&gt; between two probability distributions is a statistical distance or scoring of how the distributions differ from each other. The &lt;strong&gt;Kullback-Leibler Divergence&lt;/strong&gt;: D$_{KL} (P \mid \mid Q)$ &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; is also known as &lt;strong&gt;relative entropy&lt;/strong&gt; and intuitively it is considered as the expected &lt;strong&gt;surprise&lt;/strong&gt; from using Q as a model when the true distribution is P..&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
