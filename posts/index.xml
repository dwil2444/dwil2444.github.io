<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog Posts on home</title>
    <link>https://dwil2444.github.io/posts/</link>
    <description>Recent content in Blog Posts on home</description>
    <generator>Hugo</generator>
    <language>en</language>
    <copyright>Â© 2025 by Dane Williamson. &lt;a href=&#39;https://dwil2444.github.io/privacy&#39;&gt;Privacy policy&lt;/a&gt;.</copyright>
    <lastBuildDate>Fri, 10 Feb 2023 21:05:22 -0500</lastBuildDate>
    <atom:link href="https://dwil2444.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Information Theory</title>
      <link>https://dwil2444.github.io/posts/entropy/</link>
      <pubDate>Fri, 10 Feb 2023 21:05:22 -0500</pubDate>
      <guid>https://dwil2444.github.io/posts/entropy/</guid>
      <description>&lt;h1 align=&#34;center&#34;&gt;Brief Notes on Information Theory&lt;/h1&gt;&#xA;&lt;h3 id=&#34;surprise&#34;&gt;&#xA;  Surprise:&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#surprise&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Surprise quantifies how unexpected an observation $x$ is, based on its probability $p(x)$. Rare events (low $p(x)$) are more surprising. Mathematically, the surprise (sometimes called self-information) is defined as the logarithm of the inverse of the probability of an event:&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;s(x) = \log \left( \frac{1}{p(x)} \right) = -\log(p(x))&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;This means that less probable (or rarer) events produce more surprise because the logarithm of a smaller probability results in a larger value (after negation).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Variational Autoencoders</title>
      <link>https://dwil2444.github.io/posts/vae/</link>
      <pubDate>Tue, 10 May 2022 15:10:15 -0400</pubDate>
      <guid>https://dwil2444.github.io/posts/vae/</guid>
      <description>&lt;!-- &#xA;#### Inference&#xA;&#xA;#### Generation&#xA;&#xA;#### ELBO&#xA;&#xA;#### Reparameterization Trick --&gt;</description>
    </item>
    <item>
      <title>Maximum Likelihood Estimation</title>
      <link>https://dwil2444.github.io/posts/mle/</link>
      <pubDate>Sun, 08 May 2022 00:00:00 +0000</pubDate>
      <guid>https://dwil2444.github.io/posts/mle/</guid>
      <description>&lt;h1 align=&#34;center&#34;&gt;Maximum Likelihood Estimation&lt;/h1&gt;&#xA;&lt;h5 id=&#34;likelihood&#34;&gt;&#xA;  Likelihood&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#likelihood&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h5&gt;&#xA;&lt;p&gt;&lt;strong&gt;Likelihood&lt;/strong&gt; describes the joint probability of observed data, &lt;strong&gt;y&lt;/strong&gt;, as a function of the parameters, &lt;strong&gt;$\theta$&lt;/strong&gt;, of a statistical model. The likelihood is &lt;strong&gt;NOT&lt;/strong&gt; a &lt;a href=&#34;https://en.wikipedia.org/wiki/Probability_density_function&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;probability density function&lt;/a&gt; of the parameters themselves. Instead, it is a function that measures how likely the observed data is for different parameter values. An intuitive way to think about likelihood is that it represents how &amp;ldquo;believable&amp;rdquo; the data is under different model parameters. For example, in a normal distribution, the likelihood tells you how probable it is to observe a specific dataset given the mean and variance values of the distribution.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian Inference</title>
      <link>https://dwil2444.github.io/posts/bp/</link>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      <guid>https://dwil2444.github.io/posts/bp/</guid>
      <description>&lt;h1 align=&#34;center&#34;&gt;Bayesian Inference &lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;Bayesian Inference&lt;/strong&gt; is an approach to statistical inference which utilises Bayes&amp;rsquo; theorem to provide updates for the probability of an outcome as new information becomes available. The mathematical formulation for Bayes theorem is quite ubiquituous and no doubt you have seen it before, however it is stated here for completeness:&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;p (z \mid x) = \frac{p(x \mid z) \cdot p(z)}{p(x)}&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;Simple right? Perhaps not so much to the unitiated to whom this equation may seem very strange and as such here is a breakdown of what these terms actually mean:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Variational Inference</title>
      <link>https://dwil2444.github.io/posts/vi/</link>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      <guid>https://dwil2444.github.io/posts/vi/</guid>
      <description>&lt;h1 align=&#34;center&#34;&gt;What is Variational Inference? &lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;Variational Inference&lt;/strong&gt; is a technique used in Bayesian Statistics to approximate &lt;strong&gt;$p ( z \mid x)$&lt;/strong&gt; the conditional density of an unknown variable, &lt;strong&gt;z&lt;/strong&gt; given an observed variable, &lt;strong&gt;x&lt;/strong&gt; through optimization. To find this approximate density:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;select a family of densities, $\mathscr{D}$ over the latent variables. Each member of the family q(z) $\in \mathscr{D}$ is a candidate approximation to the true density.&lt;/li&gt;&#xA;&lt;li&gt;The optimization problem is then to find the member of this family which is closest in &lt;strong&gt;Kullback-Leibler (KL) divergence&lt;/strong&gt; to the conditional density of interest:&#xA;$$&#xA;q^{*} (z) = \underset{q(z) \in \mathscr{D}}{\text{arg min}} \quad \text{KL} (q(z) \mid \mid p(z \mid x))&#xA;$$&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;kullback-leibler-divergence&#34;&gt;&#xA;  Kullback-Leibler Divergence&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#kullback-leibler-divergence&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;p&gt;The &lt;strong&gt;divergence&lt;/strong&gt; between two probability distributions is a statistical distance or scoring of how the distributions differ from each other. The &lt;strong&gt;Kullback-Leibler Divergence&lt;/strong&gt;: D$_{KL} (P \mid \mid Q)$ &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; is also known as &lt;strong&gt;relative entropy&lt;/strong&gt; and intuitively it is considered as the expected &lt;strong&gt;surprise&lt;/strong&gt; from using Q as a model when the true distribution is P..&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
