<!DOCTYPE html>
<html lang="en">

<head>
  <title>
  Information Theory · home
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="description" content="Brief Notes on Information Theory

  Surprise:
  
    
    Link to heading
  

Surprise quantifies how unexpected an observation $x$ is, based on its probability $p(x)$. Rare events (low $p(x)$) are more surprising. Mathematically, the surprise (sometimes called self-information) is defined as the logarithm of the inverse of the probability of an event:
$$
s(x) = \log \left( \frac{1}{p(x)} \right) = -\log(p(x))
$$
This means that less probable (or rarer) events produce more surprise because the logarithm of a smaller probability results in a larger value (after negation).">
<meta name="keywords" content="">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Information Theory">
  <meta name="twitter:description" content="Brief Notes on Information Theory Surprise: Link to heading Surprise quantifies how unexpected an observation $x$ is, based on its probability $p(x)$. Rare events (low $p(x)$) are more surprising. Mathematically, the surprise (sometimes called self-information) is defined as the logarithm of the inverse of the probability of an event:
$$ s(x) = \log \left( \frac{1}{p(x)} \right) = -\log(p(x)) $$
This means that less probable (or rarer) events produce more surprise because the logarithm of a smaller probability results in a larger value (after negation).">

<meta property="og:url" content="https://dwil2444.github.io/posts/it/entropy/">
  <meta property="og:site_name" content="home">
  <meta property="og:title" content="Information Theory">
  <meta property="og:description" content="Brief Notes on Information Theory Surprise: Link to heading Surprise quantifies how unexpected an observation $x$ is, based on its probability $p(x)$. Rare events (low $p(x)$) are more surprising. Mathematically, the surprise (sometimes called self-information) is defined as the logarithm of the inverse of the probability of an event:
$$ s(x) = \log \left( \frac{1}{p(x)} \right) = -\log(p(x)) $$
This means that less probable (or rarer) events produce more surprise because the logarithm of a smaller probability results in a larger value (after negation).">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-02-10T21:05:22-05:00">
    <meta property="article:modified_time" content="2023-02-10T21:05:22-05:00">




<link rel="canonical" href="https://dwil2444.github.io/posts/it/entropy/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.b886fe0d9034709648f91f4ce178f51dd367d9350f82dd1132d54fd69bfca66f.css" integrity="sha256-uIb&#43;DZA0cJZI&#43;R9M4Xj1HdNn2TUPgt0RMtVP1pv8pm8=" crossorigin="anonymous" media="screen" />








 




<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>




<body class="preload-transitions colorscheme-light">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="https://dwil2444.github.io/">
      home
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/about">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/publications">Publications</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/teaching">Teaching</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/cv.pdf">CV</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="https://dwil2444.github.io/posts/it/entropy/">
              Information Theory
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2023-02-10T21:05:22-05:00">
                February 10, 2023
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              3-minute read
            </span>
          </div>
          
          
          
        </div>
      </header>

      <div class="post-content">
        
        <h1 align="center">Brief Notes on Information Theory</h1>
<h3 id="surprise">
  Surprise:
  <a class="heading-link" href="#surprise">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Surprise quantifies how unexpected an observation $x$ is, based on its probability $p(x)$. Rare events (low $p(x)$) are more surprising. Mathematically, the surprise (sometimes called self-information) is defined as the logarithm of the inverse of the probability of an event:</p>
<p>$$
s(x) = \log \left( \frac{1}{p(x)} \right) = -\log(p(x))
$$</p>
<p>This means that less probable (or rarer) events produce more surprise because the logarithm of a smaller probability results in a larger value (after negation).</p>
<p>For two independent events $ x $ and $y$, surprise is additive:</p>
<p>$$
s(xy) = \log \left( \frac{1}{p(x)p(y)} \right) = \log \left( \frac{1}{p(x)} \right) + \log \left( \frac{1}{p(y)} \right) = s(x) + s(y).
$$</p>
<hr />
<h3 id="average-surprise-entropy">
  Average Surprise (Entropy):
  <a class="heading-link" href="#average-surprise-entropy">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>If data is generated by a stochastic process, the <strong>expected value of surprise</strong> gives the <strong>average information content</strong>, also known as <strong>entropy</strong>. The formula for entropy is:</p>
<p>$$
H = \sum_{i=1}^n p_i \log \left( \frac{1}{p_i} \right),
$$</p>
<p>where $ p_i$ is the probability of the $i$-th outcome.</p>
<p>Simplifying using $ \log \frac{1}{p_i} = -\log(p_i) $, we get:</p>
<p>$$
H = - \sum_{i=1}^n p_i \log(p_i).
$$</p>
<ul>
<li>Essentially, we can interpret the entropy, $H$ as the expected value of surprise across all possible outcomes. Each outcomes suprise is weighted by its probability. This is why Entropy is a measure of uncertainty - higher entropy means more unpredictability or surprise in a system.</li>
</ul>
<hr />
<h3 id="intuitive-example-of-entropy">
  Intuitive Example of Entropy:
  <a class="heading-link" href="#intuitive-example-of-entropy">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Consider a fair coin flip, where there are two possible outcomes: heads or tails. The probability of each outcome is:</p>
<ul>
<li>$ p(\text{heads}) = 0.5 $</li>
<li>$ p(\text{tails}) = 0.5 $</li>
</ul>
<p>The entropy $H$ of this system can be calculated as:</p>
<p>$$
H = \sum_{i=1}^2 p_i \log \left( \frac{1}{p_i} \right)
$$</p>
<p>For the fair coin flip, we have:</p>
<p>$$
H = (0.5 \cdot \log \left( \frac{1}{0.5} \right)) + (0.5 \cdot \log \left( \frac{1}{0.5} \right))
$$</p>
<p>Since $ \log \left( \frac{1}{0.5} \right) = 1 $, this simplifies to:</p>
<p>$$
H = (0.5 \cdot 1) + (0.5 \cdot 1) = 1 \text{ bit}
$$</p>
<p>This result of 1 bit makes sense because, in this case, there is equal surprise for both heads and tails. The system is perfectly balanced and unpredictable, meaning there is maximum uncertainty.</p>
<hr />
<p>Now, consider a biased coin where one outcome is more likely than the other. For instance:</p>
<ul>
<li>$ p(\text{heads}) = 0.8 $</li>
<li>$ p(\text{tails}) = 0.2 $</li>
</ul>
<p>The entropy for this biased coin would be:</p>
<p>$$
H = (0.8 \cdot \log \left( \frac{1}{0.8} \right)) + (0.2 \cdot \log \left( \frac{1}{0.2} \right))
$$</p>
<p>This would result in a lower entropy, reflecting less uncertainty because the outcome is more predictable.</p>
<p>In summary, entropy represents the <strong>average surprise</strong> or uncertainty of a system. A fair coin flip has higher entropy (more unpredictability) than a biased coin flip.</p>
<h3 id="key-insights">
  Key Insights:
  <a class="heading-link" href="#key-insights">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<ol>
<li>Entropy is the <strong>expected surprise</strong>, measuring the uncertainty or unpredictability of a process.</li>
<li>Higher entropy implies more uncertainty, while lower entropy implies more predictable outcomes.</li>
</ol>

      </div>


      <footer>
        


        <div id="disqus_thread"></div>
<script>
  window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "mlbytes" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
    
    document.addEventListener('themeChanged', function (e) { 
        if (document.readyState == 'complete') {
          DISQUS.reset({ reload: true, config: disqus_config });
        }
    });
</script>
        
        
        
        
        
        
      </footer>
    </article>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css"
    integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
  
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"
    integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body,
      {
        delimiters: [
          {left: '$$', right: '$$', display:true},
          {left: '$', right: '$', display:false},
          {left: '\\(', right: '\\)', display: false},
          {left: '\\[', right: '\\]', display: true}
        ]
      }
    );"></script>
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
    2025
    
    ·
    
    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js" integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>
</html>
