<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>home</title>
    <link>https://dwil2444.github.io/</link>
    <description>Recent content on home</description>
    <generator>Hugo</generator>
    <language>en</language>
    <copyright>Â© 2025 by Dane Williamson. &lt;a href=&#39;https://dwil2444.github.io/privacy&#39;&gt;Privacy policy&lt;/a&gt;.</copyright>
    <lastBuildDate>Fri, 10 Feb 2023 21:05:22 -0500</lastBuildDate>
    <atom:link href="https://dwil2444.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Information Theory</title>
      <link>https://dwil2444.github.io/posts/it/</link>
      <pubDate>Fri, 10 Feb 2023 21:05:22 -0500</pubDate>
      <guid>https://dwil2444.github.io/posts/it/</guid>
      <description>&lt;h1 align=&#34;center&#34;&gt;Brief Notes on Information Theory &lt;/h1&gt;&#xA;&lt;h3 id=&#34;surprise&#34;&gt;&#xA;  Surprise:&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#surprise&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Given a stochastic process which generates data $x$, the &lt;strong&gt;surprise&lt;/strong&gt; associated with each datapoint is the reciprocal of its probability:&#xA;$$ s(x) = \frac{1}{p(x)}; $$.&lt;/li&gt;&#xA;&lt;li&gt;The lower the probability of an observation, the more &amp;ldquo;surprised&amp;rdquo; we are at seeing it. To capture the surprise of multiple independent events, we make use of the logarithm function:&#xA;$$ s(x) = \log ( \frac{1}{p(x)} )$$&#xA;$$ s(xy) = \log ( \frac{1}{p(x) \cdot p(y)} ) = \log ( \frac{1}{p(x)} ) + \log ( \frac{1}{p(y)} )= s(x) + s(y)  $$&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;average-surprise&#34;&gt;&#xA;  Average Surprise:&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#average-surprise&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Each observation has an associated probability. To get the amount of information produced by a series of inputs, we can take the &lt;strong&gt;expected value:&lt;/strong&gt;.&#xA;&lt;strong&gt;N.B.&lt;/strong&gt; The &lt;strong&gt;Expected Surprise&lt;/strong&gt; is the surprise for each outcome weighted by its probability:&#xA;$$ H = p_{1} \log (\frac{1}{p_{1}}) + p_{2} \log (\frac{1}{p_{2}})  + &amp;hellip; + p_{n} \log (\frac{1}{p_{n}})$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Variational Autoencoders</title>
      <link>https://dwil2444.github.io/posts/vae/</link>
      <pubDate>Tue, 10 May 2022 15:10:15 -0400</pubDate>
      <guid>https://dwil2444.github.io/posts/vae/</guid>
      <description>&lt;h4 id=&#34;inference&#34;&gt;&#xA;  Inference&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#inference&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;generation&#34;&gt;&#xA;  Generation&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#generation&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;elbo&#34;&gt;&#xA;  ELBO&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#elbo&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;reparameterizatio-trick&#34;&gt;&#xA;  Reparameterizatio Trick&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#reparameterizatio-trick&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h4&gt;</description>
    </item>
    <item>
      <title>Maximum Likelihood Estimation</title>
      <link>https://dwil2444.github.io/posts/mle/</link>
      <pubDate>Sun, 08 May 2022 00:00:00 +0000</pubDate>
      <guid>https://dwil2444.github.io/posts/mle/</guid>
      <description>&lt;h1 align=&#34;center&#34;&gt;Maximum Likelihood Estimation &lt;/h1&gt;&#xA;&lt;h5 id=&#34;likelihood&#34;&gt;&#xA;  Likelihood&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#likelihood&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h5&gt;&#xA;&lt;p&gt;&lt;strong&gt;Likelihood&lt;/strong&gt; describes the joint probability of observed data, &lt;strong&gt;y&lt;/strong&gt; as a function of the parameters, &lt;strong&gt;$\theta$&lt;/strong&gt; of a statistical model. The likelihood is &lt;strong&gt;NOT&lt;/strong&gt; a &lt;a href=&#34;https://en.wikipedia.org/wiki/Probability_density_function&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;probability density function&lt;/a&gt; of &lt;strong&gt;the parameters&lt;/strong&gt;. An intuitive way to consider the likelihood is as equal to the probability density of the outcome, &lt;strong&gt;y&lt;/strong&gt; when the true value of the parameter is &lt;strong&gt;$\theta$&lt;/strong&gt;.&lt;br /&gt;&#xA;&lt;strong&gt;$\mathcal{L}$ is a probability density over y NOT $\theta$.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Teaching</title>
      <link>https://dwil2444.github.io/teaching/</link>
      <pubDate>Thu, 05 May 2022 13:23:50 -0400</pubDate>
      <guid>https://dwil2444.github.io/teaching/</guid>
      <description>&lt;h2 id=&#34;courses-taught&#34;&gt;&#xA;  Courses Taught&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#courses-taught&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://yangfengji.net/uva-ml-undergrad/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS 4774 - Machine Learning (Spring 2023)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://yangfengji.net/uva-ml-course/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS 6316 - Machine Learning (Spring 2022)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;CSCI 120 - Introduction to Computer Science  (Fall 2017, Spring 2018)&lt;/li&gt;&#xA;&lt;li&gt;CSCI 150/151 - Programming I &amp;amp; Programming I Lab  (Fall 2019, Spring 2020)&lt;/li&gt;&#xA;&lt;li&gt;CSCI 250/251 - Programming II &amp;amp; Programming II Lab  (Fall 2018, Spring 2020)&lt;/li&gt;&#xA;&lt;li&gt;CSCI 287 - Data Structures  (Fall 2019, Spring 2020)&lt;/li&gt;&#xA;&lt;li&gt;CSCI 296 - Web Programming  (Fall 2017, Spring 2018)&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Bayesian Inference</title>
      <link>https://dwil2444.github.io/posts/bp/</link>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      <guid>https://dwil2444.github.io/posts/bp/</guid>
      <description>&lt;h1 align=&#34;center&#34;&gt;Bayesian Inference &lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;Bayesian Inference&lt;/strong&gt; is an approach to statistical inference which utilises Bayes&amp;rsquo; theorem to provide updates for the probability of an outcome as new information becomes available. The mathematical formulation for Bayes theorem is quite ubiquituous and no doubt you have seen it before, however it is stated here for completeness:&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;p (z \mid x) = \frac{p(x \mid z) \cdot p(z)}{p(x)}&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;Simple right? Perhaps not so much to the unitiated to whom this equation may seem very strange and as such here is a breakdown of what these terms actually mean:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Variational Inference</title>
      <link>https://dwil2444.github.io/posts/vi/</link>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      <guid>https://dwil2444.github.io/posts/vi/</guid>
      <description>&lt;h1 align=&#34;center&#34;&gt;What is Variational Inference? &lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;Variational Inference&lt;/strong&gt; is a technique used in Bayesian Statistics to approximate &lt;strong&gt;$p ( z \mid x)$&lt;/strong&gt; the conditional density of an unknown variable, &lt;strong&gt;z&lt;/strong&gt; given an observed variable, &lt;strong&gt;x&lt;/strong&gt; through optimization. To find this approximate density:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;select a family of densities, $\mathscr{D}$ over the latent variables. Each member of the family q(z) $\in \mathscr{D}$ is a candidate approximation to the true density.&lt;/li&gt;&#xA;&lt;li&gt;The optimization problem is then to find the member of this family which is closest in &lt;strong&gt;Kullback-Leibler (KL) divergence&lt;/strong&gt; to the conditional density of interest:&#xA;$$&#xA;q^{*} (z) = \underset{q(z) \in \mathscr{D}}{\text{arg min}} \quad \text{KL} (q(z) \mid \mid p(z \mid x))&#xA;$$&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;kullback-leibler-divergence&#34;&gt;&#xA;  Kullback-Leibler Divergence&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#kullback-leibler-divergence&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;p&gt;The &lt;strong&gt;divergence&lt;/strong&gt; between two probability distributions is a statistical distance or scoring of how the distributions differ from each other. The &lt;strong&gt;Kullback-Leibler Divergence&lt;/strong&gt;: D$_{KL} (P \mid \mid Q)$ &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; is also known as &lt;strong&gt;relative entropy&lt;/strong&gt; and intuitively it is considered as the expected &lt;strong&gt;surprise&lt;/strong&gt; from using Q as a model when the true distribution is P..&lt;/p&gt;</description>
    </item>
    <item>
      <title>About Me</title>
      <link>https://dwil2444.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dwil2444.github.io/about/</guid>
      <description>&lt;p&gt;I am a Computer Science PhD student at the University of Virginia, co-advised by &lt;a href=&#34;https://matthewbdwyer.github.io/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matthew Dwyer&lt;/a&gt; and &lt;a href=&#34;https://yangfengji.net/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yangfeng Ji&lt;/a&gt;.&#xA;I am a member of both the &lt;a href=&#34;https://less-lab-uva.github.io/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LESS Lab&lt;/a&gt;  and &lt;a href=&#34;https://uvanlp.org/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ILP-Lab&lt;/a&gt; .&lt;/p&gt;&#xA;&lt;h4 id=&#34;research-interests&#34;&gt;&#xA;  Research Interests&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#research-interests&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;p&gt;My research interests lie in the areas of Software Engineering, Natural Language Processing, Computer Vision and Interpretable Machine Learning. My work is focused on:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Using unsupervised learning techniques such as dimensionality reduction and clustering to interpret the latent representation space of large language models such as &lt;a href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BERT&lt;/a&gt; and Convolutional Models such as &lt;a href=&#34;https://arxiv.org/pdf/1512.03385.pdf&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ResNet&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Developing Saliency Oracles to align the regions of interest to image classifiers with human intuition.&lt;/li&gt;&#xA;&lt;li&gt;Formal Verification of robustness properties of Deep Learning models using verifier frameworks such as &lt;a href=&#34;https://github.com/dlshriver/dnnv&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DNNV&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;other-academic-interests&#34;&gt;&#xA;  Other Academic Interests&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#other-academic-interests&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;p&gt;I am interested in making the stae of the art developments in deep learning accessible to the general public. Sifting through technical jargon is a major deterrent to the unitiated and I hope to bridge the gap through my work.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Privacy policy</title>
      <link>https://dwil2444.github.io/privacy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dwil2444.github.io/privacy/</guid>
      <description>&lt;p&gt;This is where the imprint should go.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Publications</title>
      <link>https://dwil2444.github.io/publications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dwil2444.github.io/publications/</guid>
      <description>&lt;h1 id=&#34;2019&#34;&gt;&#xA;  2019&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#2019&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;!-- * **Barks, C.** &amp; Twain, B. (2021): *To bark or not to bark? A neo-woofological perspective on Shakespeare&#39;s Hamlet.* Journal of Barkology, 42 (23). DOI: [10.1234/3456789](https://doi2bib.org/bib/10.1234/3456789).&#xA;* Twain, B., **Barks, C.**, Warhowl, A. (2020): *Barking is all you need.* Proceedings of the 31th International Conference on Yapping. DOI: [10.987654/321](https://doi2bib.org/bib/10.1234/3456789).   --&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Cowart, G., &lt;strong&gt;Williamson, D.&lt;/strong&gt;, Farhat, N., &amp;amp; Lee, J.-S. (2019). &lt;a href=&#34;https://www.semanticscholar.org/paper/Do-Humans-STILL-Have-a-Monopoly-on-Creativity-or-Is-Cowart-Williamson/cfac322dcd274db4cba4b0e2dfe2e1879d0a9fa6&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do Humans STILL Have a Monopoly on Creativity or Is Creativity Overrated? HCI.&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;!-- &#xA;&#xA;# Preprints&#xA;&#xA;* **Barks, C.**, Twain, B. (2021): *Who let the dogs out, who, who?* Submitted to the *Barking: past, present, future* workshop at WoofCon 2021. barXiv: [2109.123456](https://barxiv.org/abs/2109.123456). --&gt;</description>
    </item>
  </channel>
</rss>
