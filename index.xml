<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>home</title>
    <link>https://dwil2444.github.io/</link>
    <description>Recent content on home</description>
    <generator>Hugo</generator>
    <language>en</language>
    <copyright>Â© 2025 by Dane Williamson. &lt;a href=&#39;https://dwil2444.github.io/privacy&#39;&gt;Privacy policy&lt;/a&gt;.</copyright>
    <lastBuildDate>Tue, 04 Feb 2025 18:55:35 -0500</lastBuildDate>
    <atom:link href="https://dwil2444.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://dwil2444.github.io/posts/stats/general/resampling/</link>
      <pubDate>Tue, 04 Feb 2025 18:55:35 -0500</pubDate>
      <guid>https://dwil2444.github.io/posts/stats/general/resampling/</guid>
      <description>&lt;h1 align=&#34;center&#34;&gt; Resampling Methods: Bootstrap and Cross-Validation &lt;/h1&gt;&#xA;&lt;p&gt;Resampling methods are powerful statistical techniques used to assess model performance and estimate&#xA;uncertainty in datasets. Two widely used resampling techniques are &lt;strong&gt;bootstrap&lt;/strong&gt; and &lt;strong&gt;cross-validation.&lt;/strong&gt;&#xA;These methods help mitigate issues like overfitting and provide robut estimates of model performance.&lt;/p&gt;&#xA;&lt;h1 id=&#34;bootstrap&#34;&gt;&#xA;  Bootstrap&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#bootstrap&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;what-is-bootstrap&#34;&gt;&#xA;  What is Bootstrap?&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#what-is-bootstrap&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;The bootstrap is a resampling technique that involves repeatedly drawing samples, with replacement, from&#xA;the original dataset to estimate statistical properties (e.g. variance, confidence intervals and standard&#xA;errors). It is particularly useful when analytical solutions are diificult to derive.&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>https://dwil2444.github.io/posts/stats/general/ht/</link>
      <pubDate>Tue, 04 Feb 2025 17:11:41 -0500</pubDate>
      <guid>https://dwil2444.github.io/posts/stats/general/ht/</guid>
      <description>&lt;h1 align=&#34;center&#34;&gt; Hypothesis Testing: An Overview &lt;/h1&gt;&#xA;&lt;p&gt;Hypothesis Testing is a fundamental statistical method used to make inferences or draw conclusions about a population&#xA;based on sample data. It is commonly used to test if a claim about a population is supported by data or if the observed effect is&#xA;&lt;strong&gt;statistically significant.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;key-concepts-in-hypothesis-testing&#34;&gt;&#xA;  Key Concepts in Hypothesis Testing&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#key-concepts-in-hypothesis-testing&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;hr /&gt;&#xA;&lt;h3 id=&#34;1-null-hypothesis-h_0-and-alternative-hypothesis-h_1&#34;&gt;&#xA;  1. Null Hypothesis $H_0$ and Alternative Hypothesis ($H_1$)&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#1-null-hypothesis-h_0-and-alternative-hypothesis-h_1&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Null Hypothesis ($H_0$): A statement or assumption about the population that we seek to test. It is typically a statement of &amp;ldquo;no effect&amp;rdquo; or &amp;ldquo;no difference&amp;rdquo;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Basic Probability Review</title>
      <link>https://dwil2444.github.io/posts/stats/general/basics/</link>
      <pubDate>Tue, 04 Feb 2025 13:52:07 -0500</pubDate>
      <guid>https://dwil2444.github.io/posts/stats/general/basics/</guid>
      <description>&lt;h1 align=&#34;center&#34;&gt; Probability Fundamentals &lt;/h1&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;&#xA;  Introduction&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#introduction&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Probability is the branch of mathematics that deals with the chance of events occuring. It is essential for understanding&#xA;data patterns, making predictions, and modeling uncertain events. In this post, we explore key concepts of probability,&#xA;including basic rules, Bayes&amp;rsquo; Theorem, random variables, probability distributions and more.&lt;/p&gt;&#xA;&lt;h4 id=&#34;1-basic-probablity-rules&#34;&gt;&#xA;  1. Basic Probablity Rules&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#1-basic-probablity-rules&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Probability of an Event:&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The probability of an event, A denoted as &lt;strong&gt;P(A)&lt;/strong&gt;, is a measure of the likelihood that event A will occur. The value of $P(A) \in ( 0, 1 ) $, where:&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>https://dwil2444.github.io/posts/stats/general/ci/</link>
      <pubDate>Mon, 03 Feb 2025 17:00:33 -0500</pubDate>
      <guid>https://dwil2444.github.io/posts/stats/general/ci/</guid>
      <description>&lt;h1 align=&#34;center&#34;&gt; Confidence Intervals &lt;/h1&gt;&#xA;&lt;!-- &#xA;A Confidence Interval (CI) is a range of  --&gt;</description>
    </item>
    <item>
      <title>What is Logistic Regression?</title>
      <link>https://dwil2444.github.io/posts/ml/regression/definition/</link>
      <pubDate>Mon, 03 Feb 2025 16:28:24 -0500</pubDate>
      <guid>https://dwil2444.github.io/posts/ml/regression/definition/</guid>
      <description>&lt;h1 align=&#34;center&#34;&gt;What is Logistic Regression? &lt;/h1&gt;&#xA;&lt;p&gt;Logistic Regression is a statistical method used for binary classification tasks where the goal is to predict a binary outcome (1 or 0, True or False). It is one of the&#xA;simplest algorithms in Machine Learning, but it is also one of the most widely used for problems like spam detection, disease diagnosis and customer churn prediction.&lt;/p&gt;&#xA;&lt;h4 id=&#34;definition&#34;&gt;&#xA;  Definition&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#definition&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;p&gt;Logistic regression models the probability of a binary outcome as a function of independent variables (covariates). Unlike linear regression, which is used for predicting&#xA;continuous outcomes, logistic regression is used when the outcome is categorical.&lt;br /&gt;&#xA;The model&amp;rsquo;s output is transformed by a logistic function (the sigmoid function), which ensures that the predicted values lies between 0 and 1, and is therefore a valid probability score:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Information Theory</title>
      <link>https://dwil2444.github.io/posts/it/entropy/</link>
      <pubDate>Fri, 10 Feb 2023 21:05:22 -0500</pubDate>
      <guid>https://dwil2444.github.io/posts/it/entropy/</guid>
      <description>&lt;h1 align=&#34;center&#34;&gt;Brief Notes on Information Theory&lt;/h1&gt;&#xA;&lt;h3 id=&#34;surprise&#34;&gt;&#xA;  Surprise:&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#surprise&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Surprise quantifies how unexpected an observation $x$ is, based on its probability $p(x)$. Rare events (low $p(x)$) are more surprising. Mathematically, the surprise (sometimes called self-information) is defined as the logarithm of the inverse of the probability of an event:&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;s(x) = \log \left( \frac{1}{p(x)} \right) = -\log(p(x))&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;This means that less probable (or rarer) events produce more surprise because the logarithm of a smaller probability results in a larger value (after negation).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Variational Autoencoders</title>
      <link>https://dwil2444.github.io/posts/vae/vae/</link>
      <pubDate>Tue, 10 May 2022 15:10:15 -0400</pubDate>
      <guid>https://dwil2444.github.io/posts/vae/vae/</guid>
      <description>&lt;!-- &#xA;#### Inference&#xA;&#xA;#### Generation&#xA;&#xA;#### ELBO&#xA;&#xA;#### Reparameterization Trick --&gt;</description>
    </item>
    <item>
      <title>Maximum Likelihood Estimation</title>
      <link>https://dwil2444.github.io/posts/stats/general/mle/</link>
      <pubDate>Sun, 08 May 2022 00:00:00 +0000</pubDate>
      <guid>https://dwil2444.github.io/posts/stats/general/mle/</guid>
      <description>&lt;h1 align=&#34;center&#34;&gt;Maximum Likelihood Estimation&lt;/h1&gt;&#xA;&lt;h5 id=&#34;likelihood&#34;&gt;&#xA;  Likelihood&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#likelihood&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h5&gt;&#xA;&lt;p&gt;&lt;strong&gt;Likelihood&lt;/strong&gt; describes the joint probability of observed data, &lt;strong&gt;y&lt;/strong&gt;, as a function of the parameters, &lt;strong&gt;$\theta$&lt;/strong&gt;, of a statistical model. The likelihood is &lt;strong&gt;NOT&lt;/strong&gt; a &lt;a href=&#34;https://en.wikipedia.org/wiki/Probability_density_function&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;probability density function&lt;/a&gt; of the parameters themselves. Instead, it is a function that measures how likely the observed data is for different parameter values. An intuitive way to think about likelihood is that it represents how &amp;ldquo;believable&amp;rdquo; the data is under different model parameters. For example, in a normal distribution, the likelihood tells you how probable it is to observe a specific dataset given the mean and variance values of the distribution.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Teaching</title>
      <link>https://dwil2444.github.io/teaching/</link>
      <pubDate>Thu, 05 May 2022 13:23:50 -0400</pubDate>
      <guid>https://dwil2444.github.io/teaching/</guid>
      <description>&lt;h2 id=&#34;courses-taught&#34;&gt;&#xA;  Courses Taught&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#courses-taught&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://yangfengji.net/uva-ai-undergrad/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS 6501-013 - Artificial Intelligence (Spring 2024)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://yangfengji.net/uva-nlp-grad/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS 6501 - Natural Language Processing (Fall 2023)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://yangfengji.net/uva-ml-undergrad/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS 4774 - Machine Learning (Spring 2023)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://yangfengji.net/uva-ml-course/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS 6316 - Machine Learning (Spring 2022)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;CSCI 120 - Introduction to Computer Science  (Fall 2017, Spring 2018)&lt;/li&gt;&#xA;&lt;li&gt;CSCI 150/151 - Programming I &amp;amp; Programming I Lab  (Fall 2019, Spring 2020)&lt;/li&gt;&#xA;&lt;li&gt;CSCI 250/251 - Programming II &amp;amp; Programming II Lab  (Fall 2018, Spring 2020)&lt;/li&gt;&#xA;&lt;li&gt;CSCI 287 - Data Structures  (Fall 2019, Spring 2020)&lt;/li&gt;&#xA;&lt;li&gt;CSCI 296 - Web Programming  (Fall 2017, Spring 2018)&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Bayesian Inference</title>
      <link>https://dwil2444.github.io/posts/stats/general/bp/</link>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      <guid>https://dwil2444.github.io/posts/stats/general/bp/</guid>
      <description>&lt;h1 align=&#34;center&#34;&gt;Bayesian Inference &lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;Bayesian Inference&lt;/strong&gt; is an approach to statistical inference which utilises Bayes&amp;rsquo; theorem to provide updates for the probability of an outcome as new information becomes available. The mathematical formulation for Bayes theorem is quite ubiquituous and no doubt you have seen it before, however it is stated here for completeness:&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;p (z \mid x) = \frac{p(x \mid z) \cdot p(z)}{p(x)}&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;Simple right? Perhaps not so much to the unitiated to whom this equation may seem very strange and as such here is a breakdown of what these terms actually mean:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Variational Inference</title>
      <link>https://dwil2444.github.io/posts/vae/vi/</link>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      <guid>https://dwil2444.github.io/posts/vae/vi/</guid>
      <description>&lt;h1 align=&#34;center&#34;&gt;What is Variational Inference? &lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;Variational Inference&lt;/strong&gt; is a technique used in Bayesian Statistics to approximate &lt;strong&gt;$p ( z \mid x)$&lt;/strong&gt; the conditional density of an unknown variable, &lt;strong&gt;z&lt;/strong&gt; given an observed variable, &lt;strong&gt;x&lt;/strong&gt; through optimization. To find this approximate density:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;select a family of densities, $\mathscr{D}$ over the latent variables. Each member of the family q(z) $\in \mathscr{D}$ is a candidate approximation to the true density.&lt;/li&gt;&#xA;&lt;li&gt;The optimization problem is then to find the member of this family which is closest in &lt;strong&gt;Kullback-Leibler (KL) divergence&lt;/strong&gt; to the conditional density of interest:&#xA;$$&#xA;q^{*} (z) = \underset{q(z) \in \mathscr{D}}{\text{arg min}} \quad \text{KL} (q(z) \mid \mid p(z \mid x))&#xA;$$&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;kullback-leibler-divergence&#34;&gt;&#xA;  Kullback-Leibler Divergence&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#kullback-leibler-divergence&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;p&gt;The &lt;strong&gt;divergence&lt;/strong&gt; between two probability distributions is a statistical distance or scoring of how the distributions differ from each other. The &lt;strong&gt;Kullback-Leibler Divergence&lt;/strong&gt;: D$_{KL} (P \mid \mid Q)$ &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; is also known as &lt;strong&gt;relative entropy&lt;/strong&gt; and intuitively it is considered as the expected &lt;strong&gt;surprise&lt;/strong&gt; from using Q as a model when the true distribution is P..&lt;/p&gt;</description>
    </item>
    <item>
      <title>About Me</title>
      <link>https://dwil2444.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dwil2444.github.io/about/</guid>
      <description>&lt;p&gt;I am a Computer Science PhD student at the University of Virginia, advised by &lt;a href=&#34;https://yangfengji.net/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yangfeng Ji&lt;/a&gt;.&#xA;I am a member of the &lt;a href=&#34;https://uvanlp.org/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ILP-Lab&lt;/a&gt; .&lt;/p&gt;&#xA;&lt;h4 id=&#34;research-interests&#34;&gt;&#xA;  Research Interests&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#research-interests&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;p&gt;My research interests lie in the areas of Software Engineering, Natural Language Processing, Computer Vision and Interpretable Machine Learning. My work is focused on:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Robust and interpretable machine learning, focusing on representation learning for high-dimensional data.&lt;/li&gt;&#xA;&lt;li&gt;Developing scalable frameworks that leverage deep generative models to uncover meaningful latent structures.&lt;/li&gt;&#xA;&lt;li&gt;Bridging theoretical advancements with real-world applications across complex domains.&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Privacy policy</title>
      <link>https://dwil2444.github.io/privacy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dwil2444.github.io/privacy/</guid>
      <description>&lt;p&gt;This is where the imprint should go.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Publications</title>
      <link>https://dwil2444.github.io/publications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dwil2444.github.io/publications/</guid>
      <description>&lt;h1 id=&#34;2019&#34;&gt;&#xA;  2019&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#2019&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;!-- * **Barks, C.** &amp; Twain, B. (2021): *To bark or not to bark? A neo-woofological perspective on Shakespeare&#39;s Hamlet.* Journal of Barkology, 42 (23). DOI: [10.1234/3456789](https://doi2bib.org/bib/10.1234/3456789).&#xA;* Twain, B., **Barks, C.**, Warhowl, A. (2020): *Barking is all you need.* Proceedings of the 31th International Conference on Yapping. DOI: [10.987654/321](https://doi2bib.org/bib/10.1234/3456789).   --&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Cowart, G., &lt;strong&gt;Williamson, D.&lt;/strong&gt;, Farhat, N., &amp;amp; Lee, J.-S. (2019). &lt;a href=&#34;https://www.semanticscholar.org/paper/Do-Humans-STILL-Have-a-Monopoly-on-Creativity-or-Is-Cowart-Williamson/cfac322dcd274db4cba4b0e2dfe2e1879d0a9fa6&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do Humans STILL Have a Monopoly on Creativity or Is Creativity Overrated? HCI.&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;!-- &#xA;&#xA;# Preprints&#xA;&#xA;* **Barks, C.**, Twain, B. (2021): *Who let the dogs out, who, who?* Submitted to the *Barking: past, present, future* workshop at WoofCon 2021. barXiv: [2109.123456](https://barxiv.org/abs/2109.123456). --&gt;</description>
    </item>
  </channel>
</rss>
